# Hybrid Compression Study - Go Port

**Advanced Statistical Compression Algorithm Benchmarking and Analysis Tool**

This is a comprehensive Go port of the hybrid compression study framework, designed for advanced statistical analysis where every byte and nanosecond matters. The tool provides nanosecond-precision performance metrics suitable for research and high-precision applications.

## 🚀 Features

### Compression Algorithms
- **Huffman Coding**: Optimal entropy coding with aerospace-grade metrics
- **Run-Length Encoding (RLE)**: Efficient for data with repetitive patterns
- **Lempel-Ziv-Welch (LZW)**: Dictionary-based compression with adaptive coding
- **Burrows-Wheeler Transform (BWT)**: Block-sorting transformation algorithm
- **Move-To-Front (MTF)**: Alphabet reordering transformation
- **Delta Encoding**: Predictive compression for sequential data
- **LZ77**: Sliding window dictionary compression
- **LZ78**: Explicit dictionary compression
- **Deflate**: Hybrid LZ77 + Huffman compression
- **Arithmetic Coding**: Advanced fractional bit entropy encoding

### Pipeline Compression System
- **Multi-algorithm chaining**: Chain any combination of algorithms (e.g., `bwt → mtf → rle → huffman`)
- **Custom configuration files**: User-friendly format for defining complex pipelines
- **Algorithm-specific parameters**: Fine-tune each algorithm's behavior
- **Pipeline metrics**: Detailed analysis of each stage's performance

### Advanced Statistical Metrics
- **Nanosecond-precision timing**: Ultra-precise performance measurement
- **Memory usage tracking**: Byte-level memory consumption analysis
- **CPU utilization monitoring**: Real-time CPU usage and efficiency metrics
- **I/O operation tracking**: Detailed disk I/O performance analysis
- **Statistical assessment**: Comprehensive performance evaluation for research applications

### Scientific Output
- **Comprehensive analysis reports**: Detailed performance and efficiency metrics
- **JSON output support**: Machine-readable results for further analysis
- **Advanced statistical reporting**: Research-grade status assessments
- **Resource utilization analysis**: Complete system resource profiling

## 🛠️ Installation

### Prerequisites
- Go 1.21 or later
- Unix-like system (Linux, macOS, etc.)

### Build from Source
```bash
# Clone the repository (if not already done)
git clone <repository-url>
cd hybrid-compression-study/go_port

# Install dependencies
go mod tidy

# Build the CLI tool
go build -o compress cmd/compress/main.go

# Make it executable and optionally install globally
chmod +x compress
# sudo mv compress /usr/local/bin/  # Optional: install globally
```

## 📖 Usage

### Basic Compression
```bash
# Compress a file with Huffman coding (default)
./compress input.txt

# Compress with specific algorithm
./compress -a rle data.bin
./compress -a lzw document.txt

# Specify output file
./compress -i input.txt -o compressed.huff
```

### Decompression
```bash
# Decompress a file
./compress -d compressed.huff

# Decompress with specific algorithm
./compress -d -a rle compressed.rle
```

### Advanced Options
```bash
# Verbose output with detailed metrics
./compress --verbose input.txt

# JSON output for machine processing
./compress --json -a lzw data.txt

# Multiple tries for statistical analysis (aerospace-grade precision)
./compress --tries 10 input.txt
./compress -t 5 --verbose -a rle data.bin

# Algorithm-specific parameters
./compress -a rle --escape-byte 255 --min-run-length 4 data.bin
./compress -a lzw --max-code-bits 14 text.txt
```

### Pipeline Configuration System

The tool supports a powerful configuration system that allows you to create complex multi-algorithm compression pipelines using a simple, intuitive format.

#### Configuration File Location
The main configuration file is **`experiment.conf`** (generated by `--generate-config`).

#### Generate Configuration File
```bash
# Generate example configuration file
./compress --generate-config experiment.conf

# Generate with custom name
./compress --generate-config my_pipelines.conf
```

#### Configuration File Format
The configuration format is simple and human-readable:

```conf
# Pipeline definitions: name = algorithm1+algorithm2+algorithm3
experiment1 = rle+huffman+lz77
text_optimized = bwt+mtf+rle+huffman
high_compression = delta+bwt+mtf+rle+lzw+huffman

# Algorithm-specific parameters: algorithm.parameter = value
lz77.window_size = 8192
lz77.buffer_size = 256
rle.min_run_length = 3
deflate.compression_level = 9

# Global variables
compression_level = 6
debug_mode = true
default = text_optimized
```

#### Using Configuration Files
```bash
# List all available pipelines in config file
./compress -c experiment.conf --list-pipelines

# Run specific pipeline
./compress -c experiment.conf -p text_optimized input.txt

# Run default pipeline (if specified in config)
./compress -c experiment.conf input.txt

# Verbose output with pipeline details
./compress -c experiment.conf -p experiment1 --verbose input.txt
```

#### Available Algorithms for Pipelines
- `rle` - Run-Length Encoding
- `huffman` (or `huf`) - Huffman Coding
- `lzw` - Lempel-Ziv-Welch
- `bwt` - Burrows-Wheeler Transform
- `mtf` - Move-To-Front
- `delta` - Delta Encoding
- `lz77` - LZ77 Compression
- `lz78` - LZ78 Compression
- `deflate` - Deflate (LZ77 + Huffman)
- `arithmetic` - Arithmetic Coding

#### Example Pipeline Configurations

**Text Optimization Pipeline:**
```conf
text_optimized = bwt+mtf+rle+huffman
```

**High Compression Pipeline:**
```conf
high_compression = delta+bwt+mtf+rle+lzw+huffman
```

**Dictionary-based Testing:**
```conf
dictionary_test = lz77+lz78+lzw
```

**Modern Compression:**
```conf
modern_compression = deflate
```

**Everything Combined:**
```conf
everything = delta+bwt+mtf+rle+lz77+lz78+lzw+huffman+arithmetic
```

#### Configuration Examples

**Simple Format (Your Requested Style):**
```conf
experiment1 = rle+huf+lz77
experiment2 = delta+bwt+mtf
experiment3 = lz78+arithmetic
experiment4 = deflate
```

**With Custom Parameters:**
```conf
# Pipeline definitions
custom_lz77 = lz77
custom_deflate = deflate

# Algorithm parameters
lz77.window_size = 16384
lz77.buffer_size = 512
deflate.compression_level = 9
deflate.enable_huffman = true

# Global settings
debug_mode = true
default = custom_lz77
```

### Algorithm-Specific Parameters

#### Statistical Analysis Options
- `-t, --tries N`: Number of compression runs to average for statistical analysis (default: 1)

#### Run-Length Encoding (RLE)
- `--escape-byte N`: Escape byte value (0-255, default: 0)
- `--min-run-length N`: Minimum run length to encode (default: 3)
- Config format: `rle.escape_byte = 0`, `rle.min_run_length = 3`

#### Lempel-Ziv-Welch (LZW)
- `--max-code-bits N`: Maximum code bits (9-16, default: 12)
- Config format: `lzw.max_code_bits = 14`

#### LZ77 Compression
- Config format: `lz77.window_size = 8192`, `lz77.buffer_size = 256`

#### LZ78 Compression
- Config format: `lz78.max_dict_size = 4096`, `lz78.reset_threshold = 8192`

#### Deflate Compression
- Config format: `deflate.compression_level = 9`, `deflate.enable_huffman = true`

#### Arithmetic Coding
- Config format: `arithmetic.precision = 32`, `arithmetic.adaptive_model = true`

#### Delta Encoding
- Config format: `delta.data_width = 1`, `delta.signed_data = false`

## 📊 Output Formats

### Scientific Report (Default)
The tool provides comprehensive aerospace-grade analysis:

#### Pipeline Analysis
When using configuration files with multiple algorithms, the tool provides detailed pipeline analysis:

```
🚀 AEROSPACE-GRADE PIPELINE COMPRESSION ANALYSIS
═══════════════════════════════════════════════════════════════

📊 PIPELINE SUMMARY
Pipeline: text_optimized
Input File: document.txt
Output File: document.txt.pipeline

📈 DATA INTEGRITY METRICS
Original Size:      10.24KB (10,485 bytes)
Compressed Size:    3.21KB (3,287 bytes)
Compression Ratio:  3.19x
Space Savings:      68.6% (7.20KB)
Pipeline Stages:    4 algorithms
Effectiveness:      ✅ POSITIVE

⚡ PERFORMANCE PROFILE
Total Time:         2.456ms
Throughput:         4.27 MB/s
Avg Stage Time:     0.614ms

🔧 PIPELINE STAGES
1. BWT
   Input: 10,485 bytes → Output: 10,490 bytes
   Ratio: 0.999x, Time: 0.567ms

2. MTF
   Input: 10,490 bytes → Output: 10,494 bytes
   Ratio: 0.999x, Time: 0.123ms

3. RLE
   Input: 10,494 bytes → Output: 8,321 bytes
   Ratio: 1.261x, Time: 0.089ms

4. Huffman
   Input: 8,321 bytes → Output: 3,287 bytes
   Ratio: 2.531x, Time: 1.677ms

🎯 STATISTICAL ASSESSMENT
Pipeline Complexity:  MODERATE
Memory Efficiency:    GOOD
CPU Utilization:      OPTIMAL
Analysis Quality:     ✅ EXCELLENT
```

#### Single Run Analysis
```
🚀 AEROSPACE-GRADE COMPRESSION ANALYSIS
═══════════════════════════════════════════════════════════════

📊 COMPRESSION SUMMARY
Algorithm: Huffman
Input File: example.txt
Output File: example.txt.huffman

📈 DATA INTEGRITY METRICS
Original Size:      10.24KB (10,485 bytes)
Compressed Size:    6.85KB (7,012 bytes)
Compression Ratio:  1.495x
Space Savings:      33.1% (3.39KB)
Effectiveness:      POSITIVE

⚡ PERFORMANCE PROFILE
Compression Time:   1.234ms (1,234,567 ns)
Throughput:         8.50 MB/s
Time per Byte:      117.67 ns/byte
CPU Efficiency:     8,492.31 bytes/cpu-sec

🔧 RESOURCE UTILIZATION
Peak Memory:        15.67KB
Memory Overhead:    1.493x data size
CPU Utilization:    Peak: 45.2%, Avg: 23.1%
I/O Operations:     Read: 1, Write: 1
Determinism Score:  1.000000 (1.0 = perfect)

🎯 STATISTICAL ASSESSMENT
Worst-case Latency: 1,234,567 ns
Energy Efficiency:  8.50e+06 bytes/ns
Entropy Efficiency: 0.1869
Real-time Suitable: true
Memory Constrained: true
```

#### Statistical Analysis (Multiple Tries)
When using `--tries N` with N > 1, the tool provides comprehensive statistical analysis:

```
🚀 AEROSPACE-GRADE COMPRESSION ANALYSIS (AVERAGED)
═══════════════════════════════════════════════════════════════

📊 COMPRESSION SUMMARY
Algorithm: Huffman
Input File: example.txt  
Output File: example.txt.huffman
Runs: 10 successful / 10 total (100.0% success rate)

📈 DATA INTEGRITY METRICS
Original Size:      10.24KB (10,485 bytes)
Compressed Size:    6.85KB (7,012 bytes)
Avg Compression Ratio: 1.495x ± 0.003
Space Savings:      33.1% (3.39KB)
Effectiveness:      POSITIVE

⚡ PERFORMANCE PROFILE (AVERAGED)
Avg Compression Time:   1.234ms
Time Range:            1.201ms - 1.267ms (σ = 0.018ms)
Avg Throughput:        8.50 MB/s
Throughput Range:      8.31 - 8.69 MB/s
Time per Byte:         117.67 ns/byte
CPU Efficiency:        8,492.31 bytes/cpu-sec

📊 STATISTICAL ANALYSIS
Variability Score:      0.986 (1.0 = perfect consistency)
Consistency Rating:     EXCELLENT
Time Coefficient of Variation: 0.014
Result Reliability:     AEROSPACE_GRADE
```

### JSON Output
```bash
./compress --json input.txt
```

Returns structured JSON data suitable for automated analysis and integration with other systems.

## 🏗️ Architecture

### Core Components
- **`pkg/core`**: Base interfaces and data structures
- **`pkg/algorithms`**: Compression algorithm implementations
- **`pkg/pipeline`**: Multi-algorithm pipeline system
- **`internal/performance`**: Aerospace-grade performance monitoring
- **`cmd/compress`**: Command-line interface

### Performance Monitoring
The system uses advanced performance monitoring with:
- Nanosecond-precision timing using `time.Now().UnixNano()`
- Memory tracking via Go runtime and system calls
- CPU monitoring through `/proc` filesystem (Linux) and system APIs
- Real-time resource sampling during operations

### Algorithm Implementations
Each algorithm follows the `CompressionAlgorithm` interface:
```go
type CompressionAlgorithm interface {
    Compress(ctx context.Context, data []byte) (*CompressionResult, error)
    Decompress(ctx context.Context, compressedData []byte, metadata map[string]interface{}) (*DecompressionResult, error)
    GetName() string
    GetCategory() AlgorithmCategory
    // ... other methods
}
```

## 🧪 Testing

### Unit Tests
```bash
go test ./pkg/...
go test ./internal/...
```

### Performance Benchmarks
```bash
go test -bench=. ./pkg/algorithms/
```

### Integration Tests
```bash
go test ./tests/integration/
```

## 📈 Performance Characteristics

### Huffman Coding
- **Best for**: Text files, source code, structured data
- **Compression ratio**: 1.2x - 2.5x typical
- **Speed**: Very fast encoding/decoding
- **Memory usage**: Low, proportional to alphabet size

### Run-Length Encoding
- **Best for**: Images, data with repetitive patterns
- **Compression ratio**: Highly variable (1.0x - 50x+)
- **Speed**: Extremely fast
- **Memory usage**: Minimal

### LZW
- **Best for**: General-purpose compression
- **Compression ratio**: 2x - 4x typical
- **Speed**: Moderate
- **Memory usage**: Dictionary size dependent

## 🔧 Development

### Adding New Algorithms
1. Implement the `CompressionAlgorithm` interface
2. Add constructor function
3. Include in CLI algorithm selection
4. Add tests and benchmarks

### Extending Performance Metrics
The performance monitoring system is extensible. Add new metrics by:
1. Extending `AerospacePrecisionMetrics` struct
2. Updating measurement collection in `AerospaceGradeMonitor`
3. Adding calculation logic in metric conversion functions

## 📋 Research Applications

This tool is designed for:
- **Research institutions**: Advanced algorithm performance analysis
- **Academic studies**: Comprehensive statistical compression research
- **Performance analysis**: Detailed algorithm comparison and benchmarking
- **Statistical research**: Algorithm performance comparison and analysis
- **Quality assurance**: Deterministic performance validation

## 🏆 Performance Targets

### Statistical Analysis Thresholds
- **High-performance analysis**: < 1 second total processing time
- **Memory efficiency**: < 2x data size peak memory usage
- **CPU efficiency**: < 80% peak CPU utilization
- **Determinism**: Perfect reproducibility (score = 1.0)

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Implement changes with tests
4. Ensure all benchmarks pass
5. Submit a pull request

## 📜 License

MIT License - see LICENSE file for details.

## 🔗 Related Projects

- Original Python implementation: `../src/hybrid_compression_study/`
- Academic paper: [Link to research paper]
- Benchmark datasets: `../data/test_files/`

## 🔥 Quick Reference

### Essential Commands

```bash
# Generate configuration file
./compress --generate-config experiment.conf

# List available pipelines
./compress -c experiment.conf --list-pipelines

# Run pipeline (your requested format)
./compress -c experiment.conf -p experiment1 input.txt
# Where experiment1 = rle+huf+lz77 (defined in config)

# Run with verbose output
./compress -c experiment.conf -p text_optimized --verbose input.txt

# Single algorithm (traditional way)
./compress -a huffman input.txt

# Statistical analysis (multiple runs)
./compress -c experiment.conf -p high_compression --tries 10 input.txt
```

### Configuration File Quick Start

1. **Generate example config**: `./compress --generate-config experiment.conf`
2. **Edit the config** to add your own pipelines:
   ```conf
   # Your custom experiments
   experiment1 = rle+huf+lz77
   experiment2 = delta+bwt+mtf
   experiment3 = deflate
   
   # Custom parameters
   lz77.window_size = 16384
   rle.min_run_length = 4
   
   # Set default
   default = experiment1
   ```
3. **Run your experiments**: `./compress -c experiment.conf -p experiment1 input.txt`

### Available Algorithms
- `rle`, `huffman`/`huf`, `lzw`, `bwt`, `mtf`, `delta`, `lz77`, `lz78`, `deflate`, `arithmetic`

### File Locations
- **Main config file**: `experiment.conf` (generate with `--generate-config`)
- **Executable**: `./compress` (build with `go build -o compress cmd/compress/main.go`)

## 📞 Support

For issues, questions, or contributions, please open an issue on the project repository.

---

**Built for advanced statistical analysis and performance** 📊 
